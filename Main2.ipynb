{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luperezsal/DaVincis/blob/master/Main2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Da-Vincis"
      ],
      "metadata": {
        "id": "JO0yUGtjNTip"
      },
      "id": "JO0yUGtjNTip"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [IberLEF Davincis](https://codalab.lisn.upsaclay.fr/competitions/2638)\n",
        "\n",
        "- [DaVincis Home](https://sites.google.com/view/davincis-iberlef/home)\n",
        "\n",
        "- [GitHub](https://github.com/luperezsal/DaVincis)\n",
        "\n",
        "- [Transformer](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment)\n",
        "\n",
        "\n",
        "DOCS:\n",
        "- [CLS USE](https://engati.medium.com/bert-for-sentiment-analysis-of-chatbot-conversations-fbfdc392d68a)"
      ],
      "metadata": {
        "id": "125a7nm_JtcF"
      },
      "id": "125a7nm_JtcF"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "4U1s4aQ3Zeyt",
        "outputId": "d7ba1519-c3c6-46de-e051-f3e5af746694",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4U1s4aQ3Zeyt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
            "\u001b[K     |████████████████████████████████| 308 kB 27.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 11.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.0)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.8.0-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 75.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.36)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.11.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 9.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.7.1)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.9.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 74.0 MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.1-py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 70.9 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 8.3 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.3.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (4.2.0)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=6946b4d2cdb7abc29f3010e099ab4be80b093dbf6f84adfbea947d212e298342\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.0 alembic-1.8.0 autopage-0.5.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.1 colorlog-6.6.0 optuna-2.10.0 pbr-5.9.0 pyperclip-1.8.2 stevedore-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9abd4de-1d9d-4847-8be6-e7cfc02ab7e0",
      "metadata": {
        "id": "a9abd4de-1d9d-4847-8be6-e7cfc02ab7e0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71593f6a-a329-4e9b-a9c0-5d69aa670dcf",
      "metadata": {
        "id": "71593f6a-a329-4e9b-a9c0-5d69aa670dcf"
      },
      "outputs": [],
      "source": [
        "# DATA_PATH = 'data/'\n",
        "DATA_PATH = 'https://raw.githubusercontent.com/luperezsal/DaVincis/master/data/'\n",
        "\n",
        "TRAINING_DATA_PATH = f\"{DATA_PATH}training_data/\"\n",
        "TEST_DATA_PATH = f\"{DATA_PATH}test_data/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TASK = 2\n",
        "# TASK = 2"
      ],
      "metadata": {
        "id": "dUVcLXtYVoQT"
      },
      "id": "dUVcLXtYVoQT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ef2cdb0c-493f-4587-a680-8a22330711c8",
      "metadata": {
        "id": "ef2cdb0c-493f-4587-a680-8a22330711c8"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78ceb903-fd7d-47e3-a487-ead21b24bda0",
      "metadata": {
        "id": "78ceb903-fd7d-47e3-a487-ead21b24bda0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "c54d6b3d-475c-40a5-decb-b5a715e5c12f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-15f7fb53738b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{TRAINING_DATA_PATH}train_data.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mY_train_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{TRAINING_DATA_PATH}train_labels_subtask_1.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclass_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mY_train_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{TRAINING_DATA_PATH}train_labels_subtask_2.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/training_data/train_data.csv'"
          ]
        }
      ],
      "source": [
        "text_col  = 'tweet'\n",
        "class_col = 'incident'\n",
        "\n",
        "# Train Data\n",
        "X_train = pd.read_csv(f\"{TRAINING_DATA_PATH}train_data.csv\", names = [text_col])\n",
        "Y_train_1 = pd.read_csv(f\"{TRAINING_DATA_PATH}train_labels_subtask_1.csv\", names = [class_col])\n",
        "Y_train_2 = pd.read_csv(f\"{TRAINING_DATA_PATH}train_labels_subtask_2.csv\", header = None)\n",
        "\n",
        "# Trial Data\n",
        "X_trial = pd.read_csv(f\"{TRAINING_DATA_PATH}trial_data.csv\", names = [text_col])\n",
        "Y_trial_1 = pd.read_csv(f\"{TRAINING_DATA_PATH}trial_labels_subtask_1.csv\", names = [class_col])\n",
        "Y_trial_2 = pd.read_csv(f\"{TRAINING_DATA_PATH}trial_labels_subtask_2.csv\", header = None)\n",
        "\n",
        "# Test Data\n",
        "X_trial = pd.read_csv(f\"{TRAINING_DATA_PATH}trial_data.csv\", names = [text_col])\n",
        "Y_trial_1 = pd.read_csv(f\"{TRAINING_DATA_PATH}trial_labels_subtask_1.csv\", names = [class_col])\n",
        "Y_trial_2 = pd.read_csv(f\"{TRAINING_DATA_PATH}trial_labels_subtask_2.csv\", header = None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_2 = Y_train_2.idxmax(axis=1)\n",
        "Y_trial_2 = Y_trial_2.idxmax(axis=1)"
      ],
      "metadata": {
        "id": "IemJ9WlsT1a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "6a0ff345-60b2-445b-85c1-d9f7efa1e4d7"
      },
      "id": "IemJ9WlsT1a8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c131fd421f6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY_train_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_train_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mY_trial_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_trial_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Y_train_2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85f8dfa8-991f-454a-b6d3-c8a31be7f858",
      "metadata": {
        "id": "85f8dfa8-991f-454a-b6d3-c8a31be7f858"
      },
      "source": [
        "## Subtask 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cb53dfe-cf72-4112-bf27-51cd8d57833f",
      "metadata": {
        "id": "9cb53dfe-cf72-4112-bf27-51cd8d57833f"
      },
      "outputs": [],
      "source": [
        "Y_train_1_labeled = Y_train_1.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1040ec9-90bb-4283-a383-1f2802cd1204",
      "metadata": {
        "id": "d1040ec9-90bb-4283-a383-1f2802cd1204"
      },
      "outputs": [],
      "source": [
        "boolean_mask = Y_train_1 == 0\n",
        "Y_train_1_labeled[class_col] = Y_train_1_labeled.mask(boolean_mask, 'Violent')\n",
        "\n",
        "boolean_mask = Y_train_1 == 1\n",
        "Y_train_1_labeled[class_col] = Y_train_1_labeled.mask(boolean_mask, 'Non Violent')\n",
        "\n",
        "Y_train_1_labeled = Y_train_1_labeled[class_col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "753349ae-d9f3-4401-b369-e8b511c0802c",
      "metadata": {
        "id": "753349ae-d9f3-4401-b369-e8b511c0802c"
      },
      "outputs": [],
      "source": [
        "sns.set(style=\"darkgrid\")\n",
        "histogram = sns.histplot(Y_train_1_labeled)\n",
        "histogram.tick_params(axis='x', rotation=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc9162f3-1457-433b-9944-b5e5e68b362e",
      "metadata": {
        "id": "dc9162f3-1457-433b-9944-b5e5e68b362e"
      },
      "outputs": [],
      "source": [
        "sample_index = Y_train_1_labeled[Y_train_1_labeled == 'Violent'].index[15]\n",
        "print(f\"None of the above: {X_train.loc[sample_index][text_col]}\\n\")\n",
        "\n",
        "sample_index = Y_train_1_labeled[Y_train_1_labeled == 'Non Violent'].index[15]\n",
        "print(f\"None of the above: {X_train.loc[sample_index][text_col]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07cc03db-a13e-4da7-ab48-04489c7187ef",
      "metadata": {
        "id": "07cc03db-a13e-4da7-ab48-04489c7187ef"
      },
      "source": [
        "### Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2963f4a4-9b9b-4c3b-a299-eb6ff2607d38",
      "metadata": {
        "id": "2963f4a4-9b9b-4c3b-a299-eb6ff2607d38"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import widgets\n",
        "from IPython.display import display\n",
        "\n",
        "text = widgets.Text()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a8d37f1-18f4-493c-9684-d711733f014b",
      "metadata": {
        "id": "4a8d37f1-18f4-493c-9684-d711733f014b"
      },
      "source": [
        "## Subtask 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis"
      ],
      "metadata": {
        "id": "f_mbwHWTDqGT"
      },
      "id": "f_mbwHWTDqGT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "024958a9-696e-475b-9120-a95bd2f0fcbb",
      "metadata": {
        "id": "024958a9-696e-475b-9120-a95bd2f0fcbb"
      },
      "outputs": [],
      "source": [
        "Y_train_2_labeled = Y_train_2.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89285a92-c407-4463-a292-e7d2f3b6e1c7",
      "metadata": {
        "id": "89285a92-c407-4463-a292-e7d2f3b6e1c7"
      },
      "source": [
        "A continuación se transforman las etiquetas de entrenamiento de forma `one-hot` a formato String para realizar una representación interpretable de las clases contenidas en el dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4858305a-48f4-4655-876d-3010d0a78c0f",
      "metadata": {
        "id": "4858305a-48f4-4655-876d-3010d0a78c0f"
      },
      "outputs": [],
      "source": [
        "Y_train_2_labeled[Y_train_2 == 0] = 'Accident'\n",
        "Y_train_2_labeled[Y_train_2 == 1] = 'Homicide'\n",
        "Y_train_2_labeled[Y_train_2 == 2] = 'None of the above'\n",
        "Y_train_2_labeled[Y_train_2 == 3] = 'Theft'\n",
        "Y_train_2_labeled[Y_train_2 == 4] = 'Kidnapping'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "906b5482-ff88-4dd7-b294-67bf6111f2b8",
      "metadata": {
        "id": "906b5482-ff88-4dd7-b294-67bf6111f2b8"
      },
      "outputs": [],
      "source": [
        "sns.set(style=\"darkgrid\")\n",
        "histogram = sns.histplot(Y_train_2_labeled)\n",
        "histogram.tick_params(axis='x', rotation=25)\n",
        "\n",
        "Y_train_2_labeled.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "678abd80-d53e-4f66-baa1-256588446efd",
      "metadata": {
        "id": "678abd80-d53e-4f66-baa1-256588446efd"
      },
      "outputs": [],
      "source": [
        "sample_index = Y_train_2_labeled[Y_train_2_labeled == 'Accident'].index[15]\n",
        "print(f\"Accident example: {X_train.loc[sample_index][text_col]}\\n\")\n",
        "\n",
        "sample_index = Y_train_2_labeled[Y_train_2_labeled == 'Homicide'].index[15]\n",
        "print(f\"Homicide example: {X_train.loc[sample_index][text_col]}\\n\")\n",
        "\n",
        "sample_index = Y_train_2_labeled[Y_train_2_labeled == 'None of the above'].index[15]\n",
        "print(f\"None of the above: {X_train.loc[sample_index][text_col]}\\n\")\n",
        "\n",
        "sample_index = Y_train_2_labeled[Y_train_2_labeled == 'Theft'].index[15]\n",
        "print(f\"Theft: {X_train.loc[sample_index][text_col]}\\n\")\n",
        "\n",
        "sample_index = Y_train_2_labeled[Y_train_2_labeled == 'Kidnapping'].index[15]\n",
        "print(f\"Kidnapping: {X_train.loc[sample_index][text_col]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Undersampling"
      ],
      "metadata": {
        "id": "6JGDztnyMn5a"
      },
      "id": "6JGDztnyMn5a"
    },
    {
      "cell_type": "code",
      "source": [
        "train = X_train\n",
        "train[class_col] = Y_train_1 if TASK == 2 else Y_train_2"
      ],
      "metadata": {
        "id": "LYMfCY4BtZba"
      },
      "id": "LYMfCY4BtZba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "4gm-sii6tgBQ"
      },
      "id": "4gm-sii6tgBQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "\n",
        "zero_class   = train[train[class_col] == 0]\n",
        "first_class  = train[train[class_col] == 1]\n",
        "second_class = train[train[class_col] == 2]\n",
        "third_class  = train[train[class_col] == 3]\n",
        "fourth_class = train[train[class_col] == 4]\n",
        "\n",
        "zero_class_downsampled = resample(zero_class,\n",
        "                                  replace=True,\n",
        "                                  n_samples=len(third_class),\n",
        "                                  random_state=42)\n",
        "\n",
        "first_class_downsampled = resample(first_class,\n",
        "                                    replace=True,\n",
        "                                    n_samples=len(third_class),\n",
        "                                    random_state=42)\n",
        "\n",
        "second_class_downsampled = resample(second_class,\n",
        "                                    replace=True,\n",
        "                                    n_samples=len(third_class),\n",
        "                                    random_state=42)\n",
        "\n",
        "third_class_downsampled = resample(third_class,\n",
        "                                    replace=True,\n",
        "                                    n_samples=len(third_class),\n",
        "                                    random_state=42)\n",
        "\n",
        "fourth_class_downsampled = resample(fourth_class,\n",
        "                                    replace=True,\n",
        "                                    n_samples=len(third_class),\n",
        "                                    random_state=42)\n",
        "\n",
        "\n",
        "train = pd.concat([zero_class_downsampled,\n",
        "                   first_class_downsampled,\n",
        "                   second_class_downsampled,\n",
        "                   third_class_downsampled,\n",
        "                   fourth_class_downsampled])"
      ],
      "metadata": {
        "id": "AePF5x9kM0Tc"
      },
      "id": "AePF5x9kM0Tc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cl# Transformer"
      ],
      "metadata": {
        "id": "EOh4hf6OKxHg"
      },
      "id": "EOh4hf6OKxHg"
    },
    {
      "cell_type": "code",
      "source": [
        "# instalar librerías. Esta casilla es últil por ejemplo si se ejecuta el cuaderno en Google Colab\n",
        "# Note que existen otras dependencias como tensorflow, etc. que en este caso se encontrarían ya instaladas\n",
        "%%capture\n",
        "!pip install transformers==4.2.1\n",
        "\n",
        "print('Done!')"
      ],
      "metadata": {
        "id": "X27KcadWK1R2"
      },
      "id": "X27KcadWK1R2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {}  # diccionario para agrupar configuraciones y variables para su posterior uso\n",
        "cfg['framework'] = 'tf'  # TensorFlow como framework (por cuestiones del formato en los datos)\n",
        "cfg['max_length'] = 512  # máxima longitud de secuencia recomendada por DistilBERT\n",
        "cfg['number_of_additional_features'] = 10000\n",
        "cfg['num_labels'] = 1 if (TASK == 1) else 5"
      ],
      "metadata": {
        "id": "GTYr-6aYdOfa"
      },
      "id": "GTYr-6aYdOfa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TFAutoModel, TFAutoModelForSequenceClassification\n"
      ],
      "metadata": {
        "id": "Az8dn228Kpna"
      },
      "id": "Az8dn228Kpna",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "Y_train = Y_train_1 if TASK == 1 else Y_train_2\n",
        "Y_trial = Y_trial_1 if TASK == 2 else Y_train_2\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# función auxiliar para obtener tensores de entrada al modelo a partir del texto\n",
        "def get_model_inputs(cfg, data):\n",
        "\n",
        "  encodings = cfg['tokenizer'](data,\n",
        "                               truncation = True,\n",
        "                               padding = 'max_length',\n",
        "                               max_length = cfg['max_length'],\n",
        "                               return_tensors = cfg['framework']).data\n",
        "\n",
        "  # obtener representación tf-idf de cada instancia\n",
        "  tfidf = cfg['vectorizer'].transform(data)\n",
        "  tfidf_t = tf.convert_to_tensor(tfidf.toarray(), dtype=tf.float32)\n",
        "\n",
        "  inputs = {'input_ids': encodings['input_ids'],\n",
        "            'attention_mask': encodings['attention_mask'],\n",
        "            'tfidf': tfidf_t\n",
        "            }\n",
        "          \n",
        "  return inputs\n",
        "\n",
        "# función auxiliar para realizar predicciones con el modelo\n",
        "def predict_model(model, cfg, data, pref='m'):\n",
        "  \"\"\"\n",
        "  data: list of the text to predict\n",
        "  pref: identificador para las columnas (labels_[pref], scores_[pref]_[class 1], etc.)\n",
        "  \"\"\"\n",
        "  res = {}\n",
        "  inputs = get_model_inputs(cfg, data)\n",
        "  scores = model.predict(inputs)\n",
        "  \n",
        "  # empaquetar scores dentro de un diccionario que contiene labels, scores clase 1, scores clase 2, .... El nombre de la clase se normaliza a lowercase\n",
        "  if cfg['num_labels']==1: # si es clasificación binaria, este modelo devuelve solo 1 score por instancia\n",
        "    res = {f'scores_{pref}': scores[:,0]}\n",
        "  else:\n",
        "    res = {f'scores_X_{cls}': score for cls, score in zip(cfg['label_binarizer'].classes_, [col for col in scores.T])}\n",
        "\n",
        "  # añadir datos relativos a la predicción\n",
        "  labels = cfg['label_binarizer'].inverse_transform(scores)\n",
        "  res[f'labels_X'] = labels\n",
        "\n",
        "  # convertir a dataframe ordenando las columnas primero el label y luego los scores por clase, las clases ordenadas alfabeticamente\n",
        "  res = pd.DataFrame(res, columns=sorted(list(res.keys())))\n",
        "  return res\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "\n",
        "# función auxiliar que evalúa los resultados de una clasificación\n",
        "def evaluate_model(y_true, y_pred, y_score=None, pos_label='positive'):\n",
        "  print('==== Sumario de la clasificación ==== ')\n",
        "  print(classification_report(y_true, y_pred))\n",
        "\n",
        "  print('Accuracy -> {:.2%}\\n'.format(accuracy_score(y_true, y_pred)))\n",
        "\n",
        "  # graficar matriz de confusión\n",
        "  display_labels = sorted(unique_labels(y_true, y_pred), reverse=True)\n",
        "  cm = confusion_matrix(y_true, y_pred, labels=display_labels)\n",
        "\n",
        "  z = cm[::-1]\n",
        "  x = display_labels\n",
        "  y =  x[::-1].copy()\n",
        "  z_text = [[str(y) for y in x] for x in z]\n",
        "\n",
        "  fig_cm = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')\n",
        "\n",
        "  fig_cm.update_layout(\n",
        "      height=400, width=400,\n",
        "      showlegend=True,\n",
        "      margin={'t':150, 'l':0},\n",
        "      title={'text' : 'Matriz de Confusión', 'x':0.5, 'xanchor': 'center'},\n",
        "      xaxis = {'title_text':'Valor Real', 'tickangle':45, 'side':'top'},\n",
        "      yaxis = {'title_text':'Valor Predicho', 'tickmode':'linear'},\n",
        "  )\n",
        "  fig_cm.show()\n",
        "\n",
        "\n",
        "  # curva roc (definido para clasificación binaria)\n",
        "  fig_roc = None\n",
        "  if y_score is not None:\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=pos_label)\n",
        "    fig_roc = px.area(\n",
        "        x=fpr, y=tpr,\n",
        "        title = f'Curva ROC (AUC={auc(fpr, tpr):.4f})',\n",
        "        labels=dict(x='Ratio Falsos Positivos', y='Ratio Verdaderos Positivos'),\n",
        "        width=400, height=400\n",
        "    )\n",
        "    fig_roc.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1)\n",
        "\n",
        "    fig_roc.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
        "    fig_roc.update_xaxes(constrain='domain')\n",
        "    \n",
        "    fig_roc.show()\n"
      ],
      "metadata": {
        "id": "GTQGq3kh49iD"
      },
      "id": "GTQGq3kh49iD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_graph(cfg):\n",
        "  # transformer = TFDistilBertModel.from_pretrained(cfg['transformer_model_name'], return_dict=False).distilbert\n",
        "  transformer = TFAutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
        "                                                                     \n",
        "                                                                     output_hidden_states = True)\n",
        "\n",
        "\n",
        "  input_ids = tf.keras.layers.Input(shape=(cfg['max_length'],), name='input_ids', dtype='int32')\n",
        "\n",
        "\n",
        "  input_masks = tf.keras.layers.Input(shape=(cfg['max_length'],), name='attention_mask', dtype='int32') \n",
        "\n",
        "\n",
        "  input_tfidf = tf.keras.layers.Input(shape = (cfg['number_of_additional_features'],),\n",
        "                                      name = 'tfidf',\n",
        "                                      dtype = 'float32') \n",
        "\n",
        "  transformer_output = transformer(input_ids,\n",
        "                                    attention_mask = input_masks)\n",
        "\n",
        "  last_hidden_state = transformer_output.hidden_states[-1]\n",
        "\n",
        "  transformes_cls_embedding = tf.keras.layers.Lambda(lambda seq: seq[:,0,:], name='lambda')(last_hidden_state)\n",
        "\n",
        "  # return last_hidden_state\n",
        "  # concatenar embedding del token [CLS] con el vector de rasgos adicionales.\n",
        "  features = tf.keras.layers.concatenate([transformes_cls_embedding, input_tfidf], name='concatenate')\n",
        "\n",
        "  # establecer algunos hiper-parámetros del modelo\n",
        "  initializer_range = 0.02\n",
        "  hiden_units = 768\n",
        "  seq_classif_dropout=0.2\n",
        "  initializer = tf.keras.initializers.TruncatedNormal(stddev=initializer_range)\n",
        "\n",
        "  # crear pre_classifier, establecer como su entrada los rasgos concatenados (features).\n",
        "  pre_classifier = tf.keras.layers.Dense(hiden_units, kernel_initializer=initializer, activation='relu', name='pre_classifier')(features) \n",
        "\n",
        "  # crear dropout layer y establecer como su entrada la salida de pre_classifier.\n",
        "  dropout_layer = tf.keras.layers.Dropout(rate=seq_classif_dropout, name='dropout')(pre_classifier)  \n",
        "\n",
        "  # crear classifier layer y establecer como su entrada la salida de la capa dropout.\n",
        "  classifier = tf.keras.layers.Dense(cfg['num_labels'], kernel_initializer=initializer, name='classifier')(dropout_layer)\n",
        "\n",
        "  return input_ids, input_masks, input_tfidf, classifier\n",
        "\n",
        "print('Done!')\n",
        "\n",
        "# something = get_model_graph(cfg)"
      ],
      "metadata": {
        "id": "xhaLQgYbFc3h"
      },
      "id": "xhaLQgYbFc3h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instanciar tokenizador, tf-idf vectorizer, etc."
      ],
      "metadata": {
        "id": "ob0NQPcu712b"
      },
      "id": "ob0NQPcu712b"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "cfg['tokenizer'] = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "\n",
        "# instanciar TfidfVectorizer\n",
        "cfg['vectorizer'] = TfidfVectorizer(stop_words = stopwords.words('spanish'),\n",
        "                                    max_features = cfg['number_of_additional_features'])\n",
        "\n",
        "# entrenar TfidfVectorizer\n",
        "cfg['vectorizer'].fit(X_train[text_col].to_list())\n",
        "\n",
        "cfg['label_binarizer'] = preprocessing.LabelBinarizer() \n",
        "\n",
        "# entrenar LabelBinarizer\n",
        "cfg['label_binarizer'].fit(Y_train)"
      ],
      "metadata": {
        "id": "QzY8BCX470X_"
      },
      "id": "QzY8BCX470X_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformar a onehot"
      ],
      "metadata": {
        "id": "nKM-VJk-DOcT"
      },
      "id": "nKM-VJk-DOcT"
    },
    {
      "cell_type": "code",
      "source": [
        "comments_labels = train_data1[['Tag1','Tag2','Tag3','Tag4','Tag5']]"
      ],
      "metadata": {
        "id": "HZO87svV8wsx"
      },
      "id": "HZO87svV8wsx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtener codificación one-hot\n",
        "train_blabels = cfg['label_binarizer'].transform(Y_train)\n",
        "trial_blabels = cfg['label_binarizer'].transform(Y_trial)\n",
        "\n",
        "train_blabels_t = tf.convert_to_tensor(train_blabels, dtype='int32')\n",
        "trial_blabels_t = tf.convert_to_tensor(trial_blabels, dtype='int32')\n",
        "\n",
        "# obtener diccionarios representando las entradas del modelo\n",
        "train_inputs = get_model_inputs(cfg, X_train[text_col].to_list())\n",
        "trial_inputs = get_model_inputs(cfg, X_trial[text_col].to_list())"
      ],
      "metadata": {
        "id": "4unKS7Xk90kO"
      },
      "id": "4unKS7Xk90kO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_model(input_ids, input_masks, input_tfidf, classifier):\n",
        "  # definir algoritmo de optimización\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "\n",
        "  # definir función loss. Debe cuidarse que sea coherente con la salida esperada del modelo (vector de num_labels elementos)\n",
        "  # y el formato de los ejemplos (vector one-hot de num_labels componentes para codificar las categorías)\n",
        "  if (TASK == 1):\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "  else :\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True) \n",
        "\n",
        "  # crear el modelo\n",
        "  model = tf.keras.Model(inputs=[input_ids, input_masks, input_tfidf], outputs=classifier, name='distilbert-custom') # conectar todos los nodos en un modelo\n",
        "\n",
        "  # compilar el modelo, indicando otras métricas que se desee monitorear\n",
        "  # La métrica debe ser apropiada para el tipo de problema (clasificación binaria o multiclase)\n",
        "  metric = ['binary_accuracy'] if (TASK == 1) else ['categorical_accuracy']\n",
        "\n",
        "  model.compile(optimizer = optimizer,\n",
        "                loss = loss,\n",
        "                metrics = metric)\n",
        "\n",
        "  return model\n",
        "\n",
        "print('Done!')"
      ],
      "metadata": {
        "id": "ff-O7cKT1Txj"
      },
      "id": "ff-O7cKT1Txj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, input_masks, input_tfidf, classifier = get_model_graph(cfg)\n",
        "model = configure_model(input_ids, input_masks, input_tfidf, classifier)\n",
        "\n",
        "# imprimir sumario del modelo \n",
        "model.summary()\n",
        "\n",
        "# graficar el modelo (opcional)\n",
        "# tf.keras.utils.plot_model(model) \n",
        "\n",
        "print('Done!')"
      ],
      "metadata": {
        "id": "l7i6gtDR4GVH"
      },
      "id": "l7i6gtDR4GVH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "0j8umDGv4PF0"
      },
      "id": "0j8umDGv4PF0"
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_inputs))\n",
        "type(train_inputs['input_ids'])\n",
        "train_inputs.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "BAJNLnE5aC3e",
        "outputId": "ca0bd005-91ae-41c2-c951-c2b410a88881"
      },
      "id": "BAJNLnE5aC3e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a479b37f31b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_inputs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_inputs.keys())\n",
        "train_blabels_t.shape\n",
        "train_blabels_t\n",
        "train_inputs['input_ids']\n",
        "# X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "LeiGS2ySttBj",
        "outputId": "9a6976eb-4724-4c30-9c5e-55ecbd7df972"
      },
      "id": "LeiGS2ySttBj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6a967a220dea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_blabels_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_blabels_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# X_train.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_inputs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# gpu_options = tf.GPUOptions(allow_growth=True)\n",
        "# session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
        "\n",
        "epochs_max = 1\n",
        "epochs_to_save = 1\n",
        "batch_size = 8\n",
        "\n",
        "# ciclo de entrenamiento y guardar checkpoints\n",
        "for epoch in tqdm(range(0, epochs_max, epochs_to_save)):\n",
        "    print('Training model, epochs {0} - {1}'.format(epoch+1, epoch+epochs_to_save))\n",
        "    \n",
        "    # entrenar el modelo. Opcionalmente, se puede suministrar datos de validación => validation_data=(val_inputs,val_blabels_t )\n",
        "    model.fit(train_inputs,\n",
        "              y = train_blabels_t,\n",
        "              epochs = epochs_to_save,\n",
        "              batch_size = batch_size,\n",
        "              validation_data = (trial_inputs, trial_blabels_t))\n",
        "    \n",
        "    model.save_weights(f\"task_{TASK}\", save_format=\"tf\")\n",
        "    \n",
        "print('Done!')"
      ],
      "metadata": {
        "id": "Ay9s_omP4HJX"
      },
      "id": "Ay9s_omP4HJX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predecir los datos de entrenamiento\n",
        "data = X_train\n",
        "true_labels = Y_train\n",
        "\n",
        "m_pred = predict_model(model, cfg, data[text_col].to_list(), pref='m')\n",
        "\n",
        "evaluate_model(m_pred['labels_X'], true_labels)  # notar que en este caso se no suministran los scores\n",
        "\n",
        "print('Done!')"
      ],
      "metadata": {
        "id": "k1gex6orJIpV"
      },
      "id": "k1gex6orJIpV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = X_trial\n",
        "true_labels = Y_trial_2\n",
        "\n",
        "m_pred = predict_model(model, cfg, data[text_col].to_list(), pref='m')\n",
        "\n",
        "evaluate_model(m_pred['labels_X'], true_labels)  # notar que en este caso se no suministran los scores\n",
        "\n",
        "print('Done!')"
      ],
      "metadata": {
        "id": "WBsbJRpeo3cW"
      },
      "id": "WBsbJRpeo3cW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = get_model_inputs(cfg, data[text_col].to_list())\n",
        "scores = model.predict(inputs)"
      ],
      "metadata": {
        "id": "-W21E76F4HMQ"
      },
      "id": "-W21E76F4HMQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores.shape\n",
        "res = {f'scores_X_{cls}': score for cls, score in zip(cfg['label_binarizer'].classes_, [col for col in scores.T])}\n"
      ],
      "metadata": {
        "id": "WbsIcBNe4HPG"
      },
      "id": "WbsIcBNe4HPG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res['scores_X_0']"
      ],
      "metadata": {
        "id": "MQuuNXhN4HR0"
      },
      "id": "MQuuNXhN4HR0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(m_pred['labels_X'], true_labels)  # notar que en este caso se no suministran los scores\n"
      ],
      "metadata": {
        "id": "SIjv23YT4HUj"
      },
      "id": "SIjv23YT4HUj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "94Pd7wdn4HXA"
      },
      "id": "94Pd7wdn4HXA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = {}\n",
        "inputs = get_model_inputs(cfg, data[text_col].to_list())\n",
        "scores = model.predict(inputs)\n",
        "  \n",
        "# empaquetar scores dentro de un diccionario que contiene labels, scores clase 1, scores clase 2, .... El nombre de la clase se normaliza a lowercase\n",
        "if cfg['num_labels']==1: # si es clasificación binaria, este modelo devuelve solo 1 score por instancia\n",
        "  res = {f'scores_{pref}': scores[:,0]}\n",
        "else:\n",
        "  res = {f'scores_X_{cls}': score for cls, score in zip(cfg['label_binarizer'].classes_, [col for col in scores.T])}"
      ],
      "metadata": {
        "id": "rXpTT08j4HZk"
      },
      "id": "rXpTT08j4HZk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# añadir datos relativos a la predicción\n",
        "labels = cfg['label_binarizer'].inverse_transform(scores)\n"
      ],
      "metadata": {
        "id": "fIXY2OqJRius"
      },
      "id": "fIXY2OqJRius",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res[f'labels_X'] = labels"
      ],
      "metadata": {
        "id": "DIPav2INRpot"
      },
      "id": "DIPav2INRpot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg['label_binarizer'].inverse_transform(labels[:])"
      ],
      "metadata": {
        "id": "Q0HRvu_7SRvL"
      },
      "id": "Q0HRvu_7SRvL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convertir a dataframe ordenando las columnas primero el label y luego los scores por clase, las clases ordenadas alfabeticamente\n",
        "res = pd.DataFrame(res, columns=sorted(list(res.keys())))"
      ],
      "metadata": {
        "id": "z2U_5OcNRqkN"
      },
      "id": "z2U_5OcNRqkN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_pred.labels_X.value"
      ],
      "metadata": {
        "id": "nKUmNIZWaUIY"
      },
      "id": "nKUmNIZWaUIY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    keras.backend.clear_session()\n",
        "    train_x, valid_x, train_y, valid_y = train_test_split(train1_x, train1_y, train_size=0.8, test_size=0.2,\n",
        "                                                                random_state=42)\n",
        "    #optimum number of hidden layers\n",
        "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
        "    model = keras.Sequential()\n",
        "    for i in range(n_layers):\n",
        "        #optimum number of hidden nodes\n",
        "        num_hidden = trial.suggest_int(f'n_units_l{i}', 48, len(sentence_embeddings[0]), log=True)\n",
        "        #optimum activation function\n",
        "        model.add(keras.layers.Dense(num_hidden, input_shape=(len(sentence_embeddings[0]),),\n",
        "                               activation=trial.suggest_categorical(f'activation{i}', ['relu', 'linear','swish'])))\n",
        "        #optimum dropout value\n",
        "        model.add(keras.layers.Dropout(rate = trial.suggest_float(f'dropout{i}', 0.0, 0.6))) \n",
        "\n",
        "\n",
        "model.add(keras.layers.Dense(5,activation=tf.keras.activations.sigmoid)) #output Layer\n",
        "val_ds = (valid_x,valid_y)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,patience=1,min_lr=1e-05,verbose=0)\n",
        "early_stoping = EarlyStopping(monitor=\"val_loss\",min_delta=0,patience=5,verbose=0,mode=\"auto\", baseline=None,restore_best_weights=True)\n",
        "model.compile(loss='binary_crossentropy',metrics='categorical_crossentropy', optimizer='Adam')\n",
        "#optimum batch size\n",
        "histroy = model.fit(train_x,train_y, validation_data=val_ds,epochs=200,callbacks=[reduce_lr,early_stoping],verbose=0,\n",
        "                    batch_size=trial.suggest_int('size', 8, 128))\n",
        "return min(histroy.history['val_loss'])"
      ],
      "metadata": {
        "id": "50O_Zn_-8KS4"
      },
      "id": "50O_Zn_-8KS4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:TFM]",
      "language": "python",
      "name": "conda-env-TFM-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "colab": {
      "name": "Main2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}